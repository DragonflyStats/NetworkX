Implementation of Robust Regression
======================================
When fitting a least squares regression, we might find some outliers or high leverage data points. 
We have decided that these data points are not data entry errors, neither they are from a different 
population than most of our data. So we have no proper reason to exclude them from the analysis. 
Robust regression might be a good strategy since it is a compromise between excluding these points 
entirely from the analysis and including all the data points and treating all them equally in OLS 
regression. The idea of robust regression is to weigh the observations differently based on how well 
behaved these observations are. 
The idea of robust regression is to weigh the observations differently based on how well behaved 
these observations are. Roughly speaking, it is a form of weighted and reweighted least squares 
regression (i.e. a two step process , first fitting a linear model, then a robust model to correct for the 
influence of outliers). 
Robust regression is done by iterated re-weighted least squares (IRLS). The rlm command in the 
MASS package command implements several versions of robust regression. 
There are several weighting functions that can be used for IRLS. We are going to first use the Huber 
weights in this example. We will then look at the final weights created by the IRLS process. This can 
be very useful. 
Also we will look at an alternative weighting approach to Huber’s weighting – called Bisquare 
weighting. 
 
In Huber weighting, observations with small residuals get a weight of 1 and the larger the residual, 
the smaller the weight. This is defined by the weight function 
